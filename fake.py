# -*- coding: utf-8 -*-
"""Fake.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pqs8DSjJlpVplI8znJLLmyYGVG1Lxgsi
"""

!pip install tomotopy

"""#Initial Imports#"""

import sys
import pandas as pd
import tomotopy as tp
import spacy
import numpy as np
import pandas as pd
import gensim
from gensim.models.phrases import Phrases, Phraser
from bs4 import BeautifulSoup             
import re
from sklearn.feature_extraction.text import CountVectorizer
from google.colab import drive

spacy.cli.download("en_core_web_md")
nlp = spacy.load('en_core_web_md')

"""#Load Documents#"""

drive.mount('/content/drive')
fpath='/content/drive/MyDrive/'
fake=pd.read_csv(fpath+'Fake.csv', nrows = 20000)
fake.columns

fake.head()

fake.drop('subject', inplace=True, axis=1)
fake.drop('date', inplace=True, axis=1)
fake.drop('title', inplace=True, axis=1)

print("The shape of our data:",fake.shape,"\n")
print("Our column names are:",fake.columns.values)

"""#Cleaning of Collection#"""

def text_to_words( raw_text ):
    # 1. Remove HTML
    text = BeautifulSoup(raw_text).get_text() 

    # 2. Remove non-letters        
    letters_only = re.sub("[^a-zA-Z]", " ", text) 

    # 3. Convert to lower case, split into individual words
    words = letters_only.lower().split()                             
    
    # 4. In Python, searching a set is much faster than searching
    #   a list, so convert the stop words to a set
    # stops = set(stopwords.words("english"))                  
    # 
    # 5. Remove stop words
    meaningful_words = [w for w in words]   
    #
    # 6. Join the words back into one string separated by space, 
    # and return the result.
    return( " ".join( meaningful_words ))

"""#Clean Loop#"""

num_of_texts = fake["text"].size

print("Number of Documents: ", num_of_texts)

clean_fake_texts = []

for i in range(0, num_of_texts):

  if((i+1)%1000 == 0):
    print("Text %d of %d\n" % (i+1, num_of_texts))

  clean_fake_texts.append( text_to_words( fake["text"][i]))

"""#Bag of Words#"""

vectorizer = CountVectorizer(analyzer = "word",   \
                             tokenizer = None,    \
                             preprocessor = None, \
                             stop_words = None,   \
                             max_features = 5000) 

train_data_features = vectorizer.fit_transform(clean_fake_texts)

train_data_features = train_data_features.toarray()

"""#Vocab#"""

vocab = vectorizer.get_feature_names()
print (vocab)
print("\n")

dist = np.sum(train_data_features, axis=0)

for tag, count in zip(vocab, dist):
  print(count, tag)

"""#Statistics#"""



"""#WordCloud#"""

