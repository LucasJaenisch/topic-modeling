{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NPL_fakenews.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#NPL Fake News DataSet#\n",
        "\n",
        "Lucas Jaenisch Lopes"
      ],
      "metadata": {
        "id": "ii3xFS-ktmX9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AkCJm1NjMjzh",
        "outputId": "f76d164e-29ac-405c-b955-e2bb5e25b68e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tomotopy in /usr/local/lib/python3.7/dist-packages (0.12.3)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tomotopy) (1.21.6)\n"
          ]
        }
      ],
      "source": [
        "!pip install tomotopy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Initial Imports#"
      ],
      "metadata": {
        "id": "mFsp4riP0eDW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import spacy\n",
        "import pandas as pd\n",
        "import tomotopy as tp\n",
        "import numpy as np\n",
        "import gensim  \n",
        "import re\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from google.colab import drive\n",
        "\n",
        "spacy.cli.download(\"en_core_web_md\")\n",
        "nlp = spacy.load('en_core_web_md')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wLZSQukwMkyb",
        "outputId": "3c5c8e3f-f850-4a31-e003-05193c0df7b0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_md')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Load Documents#"
      ],
      "metadata": {
        "id": "dU7Vfngk0guF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')\n",
        "fpath='/content/drive/MyDrive/'\n",
        "df=pd.read_csv(fpath+'Fake.csv', nrows = 2000)\n",
        "df.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1l2IIwWKMoZJ",
        "outputId": "0858018d-cadd-474a-c09f-f21eac839422"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['title', 'text', 'subject', 'date'], dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vq8bi3neNzoj",
        "outputId": "05e02fb1-d85e-4949-8ca5-47b6b02c89dc"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 2000 entries, 0 to 1999\n",
            "Data columns (total 4 columns):\n",
            " #   Column   Non-Null Count  Dtype \n",
            "---  ------   --------------  ----- \n",
            " 0   title    2000 non-null   object\n",
            " 1   text     2000 non-null   object\n",
            " 2   subject  2000 non-null   object\n",
            " 3   date     2000 non-null   object\n",
            "dtypes: object(4)\n",
            "memory usage: 62.6+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop('subject', inplace=True, axis=1)\n",
        "df.drop('date', inplace=True, axis=1)\n",
        "df.drop('title', inplace=True, axis=1)"
      ],
      "metadata": {
        "id": "TAfz7v14N2X5"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#NLP#"
      ],
      "metadata": {
        "id": "E9-QivauhMw5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_average_feature_length(docs, feature):\n",
        "  total_size = 0\n",
        "  max_size = 0\n",
        "  min_size = 999\n",
        "\n",
        "  # print(docs)\n",
        "  # print(feature)\n",
        "\n",
        "  for i in docs:\n",
        "    length = len(i)\n",
        "    total_size += length\n",
        "  \n",
        "    if length > max_size:\n",
        "      max_size = length\n",
        "\n",
        "    if length < min_size:\n",
        "      min_size = length\n",
        "\n",
        "  print(f\"\\n{'-'*15} {feature} {'-'*15}\")\n",
        "  print(f\"Average length: {int(total_size / len(docs))}\")\n",
        "  print(f\"Max length: {max_size}\")\n",
        "  print(f\"Min length: {min_size}\")\n",
        "  print(f\"Total length (not unique): {total_size}\")\n",
        "  return int(total_size / len(docs))\n",
        "\n",
        "avg_text_size = get_average_feature_length(df['text'], 'text')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cQOEbiOnhMFB",
        "outputId": "2611e5ee-44ba-441d-925f-94e0122a8486"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--------------- text ---------------\n",
            "Average length: 2513\n",
            "Max length: 8269\n",
            "Min length: 359\n",
            "Total length (not unique): 5027750\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#HTML to plain text#\n"
      ],
      "metadata": {
        "id": "V2AYXr3Ykbz4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup as soup\n",
        "\n",
        "clean_text = []\n",
        "r = len(df)\n",
        "\n",
        "for f in df.columns:\n",
        "  if f == 'text':\n",
        "    for i in df[f][:r]:\n",
        "      clean_text.append((soup(i, \"lxml\").text, len(i)))"
      ],
      "metadata": {
        "id": "uSb3GvMKkbMg"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# remove objects where body length is greater than the avg body length\n",
        "\n",
        "new_text = []\n",
        "\n",
        "for i in range(r):\n",
        "  if not clean_text[i][1] > avg_text_size:\n",
        "    new_text.append(clean_text[i][0])"
      ],
      "metadata": {
        "id": "lL4_-c0ik_Gh"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "NXel1Eanlsjy",
        "outputId": "907c750e-e676-45e6-f306-1d418122cfee"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                text\n",
              "0  Donald Trump just couldn t wish all Americans ...\n",
              "1  House Intelligence Committee Chairman Devin Nu...\n",
              "2  On Friday, it was revealed that former Milwauk...\n",
              "3  On Christmas day, Donald Trump announced that ...\n",
              "4  Pope Francis used his annual Christmas Day mes..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f81fff8b-994a-422a-98a4-6a8c1537dbec\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Donald Trump just couldn t wish all Americans ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>House Intelligence Committee Chairman Devin Nu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>On Friday, it was revealed that former Milwauk...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>On Christmas day, Donald Trump announced that ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Pope Francis used his annual Christmas Day mes...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f81fff8b-994a-422a-98a4-6a8c1537dbec')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-f81fff8b-994a-422a-98a4-6a8c1537dbec button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-f81fff8b-994a-422a-98a4-6a8c1537dbec');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Rejoin all remaining rows to the DataFrame\n",
        "clean_df = pd.DataFrame()\n",
        "clean_df['text'] = new_text\n",
        "\n",
        "df = clean_df"
      ],
      "metadata": {
        "id": "yDT5JD4al02J"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Stop Words#"
      ],
      "metadata": {
        "id": "zf7NIN7MmPKw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spacy.cli.download(\"en_core_web_md\")\n",
        "nlp = spacy.load('en_core_web_md')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pF6-bXWrmOdR",
        "outputId": "f3172905-d636-4124-ace7-5d73ef56b240"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_md')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_to_string(df, feature) -> str:\n",
        "  output = []\n",
        "  for i in range(len(df[feature])):\n",
        "    output.append(df[feature][i].strip())\n",
        "  return output\n",
        "\n",
        "texts = convert_to_string(df, 'text')\n",
        "\n",
        "dlemma = []\n",
        "for i in texts:\n",
        "  tdoc=nlp(i.lower())\n",
        "  lm=\" \".join([token.lemma_ for token in tdoc  if not(token.pos_ == 'NUM' or token.is_stop == True or token.is_digit == True or token.is_punct == True or token.lemma == False)])\n",
        "  dlemma.append(lm)\n",
        "\n",
        "for d1, d2 in zip(texts, dlemma):\n",
        "  print(d1, '\\n=>', d2)\n",
        "  print()\n"
      ],
      "metadata": {
        "id": "PStvKEA_mSWy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Bigrams#"
      ],
      "metadata": {
        "id": "57aINLqRmfCA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models.phrases import Phrases, Phraser\n",
        "\n",
        "import gensim.utils"
      ],
      "metadata": {
        "id": "az9-N1LSmgT5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dtoken  = [gensim.utils.simple_preprocess(d, deacc= True, min_len=3) for d in dlemma]\n",
        "phrases = Phrases(dtoken, min_count=6, threshold=15)\n",
        "bigram  = Phraser(phrases)\n",
        "bdocs   = [bigram[d] for d in dtoken]\n",
        "\n",
        "[print(i) for i in bdocs]"
      ],
      "metadata": {
        "id": "QaKZkPxOmhWI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bigrams = []\n",
        "\n",
        "for i in bdocs:\n",
        "  for j in i:\n",
        "    bigrams.append(j)\n",
        "    if '_' in j:\n",
        "      print(j)\n",
        "bigramized = bigrams"
      ],
      "metadata": {
        "id": "KGhdcAMHmouI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(bigramized)"
      ],
      "metadata": {
        "id": "sZ1PEFGifviT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Removing Dups#"
      ],
      "metadata": {
        "id": "kHD4B04fn2RA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "uniques = set(bigramized)\n",
        "uniques = dict(zip(range(len(uniques)), uniques))\n",
        "\n",
        "print(\"Total bigrams: \", len(bigramized))\n",
        "print(\"Unique bigrams: \",  len(uniques))"
      ],
      "metadata": {
        "id": "eJpR_S4Ln6NI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "uniques.values()"
      ],
      "metadata": {
        "id": "u4lnd45Ooa7Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Tokenize#"
      ],
      "metadata": {
        "id": "RqhxYHCBoyog"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(dlemma):\n",
        "  max_size = len(dlemma); count = 0\n",
        "  output = []\n",
        "  \n",
        "  for i in dlemma:\n",
        "    if count % 200 == 0:\n",
        "      print(f\"{count}/{max_size}\")\n",
        "    \n",
        "    aux = []; doc = nlp(i)\n",
        "\n",
        "    for token in doc:\n",
        "      if token.pos_ in ['NOUN', 'PROPN']:\n",
        "        aux.append(token.lemma_)\n",
        "    output.append(aux)\n",
        "    count += 1\n",
        "  return output\n",
        "        \n",
        "tokens = tokenize(uniques.values())\n",
        "tokens[:10]"
      ],
      "metadata": {
        "id": "8SBj-PkCo064"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# remove empty objects from the tokens\n",
        "\n",
        "print(\"Old number of Tokens: \", len(tokens))\n",
        "clean_tokens = []\n",
        "for t in tokens:\n",
        "  if t != []:\n",
        "    clean_tokens.append(t)\n",
        "tokens = clean_tokens\n",
        "print(\"New number of Tokens: \", len(tokens))"
      ],
      "metadata": {
        "id": "P4VapL0po63I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# [print(d) for d in tokens]"
      ],
      "metadata": {
        "id": "4Iq60ZYctHWg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Metrics#"
      ],
      "metadata": {
        "id": "vSJ2gPyCtgrI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "number_of_docs = len(tokens)\n",
        "number_of_docs"
      ],
      "metadata": {
        "id": "k5dDSYX4tiXP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def stats_about_the_docs(docs):\n",
        "  shortest_doc, largest_doc = \"_\"*100, \"\"\n",
        "  max_size, min_size = 0, 999\n",
        "  total_size = 0\n",
        "\n",
        "  for i in docs:\n",
        "      length = len(i[0])\n",
        "      total_size += length\n",
        "\n",
        "      if length > max_size:\n",
        "        max_size = length\n",
        "        largest_doc = i[0]\n",
        "\n",
        "      if length < min_size:\n",
        "        min_size = length\n",
        "        shortest_doc = i[0]\n",
        "\n",
        "  print(f\"Average word length: {int(total_size / len(docs))}\")\n",
        "  print(f\"\\nMax length {max_size} : {largest_doc}\")\n",
        "  print(f\"Min length  {min_size} : {shortest_doc}\")\n",
        "\n",
        "stats_about_the_docs(tokens)"
      ],
      "metadata": {
        "id": "-DEqW6amtkLf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_single_string(tokens):\n",
        "  output = ''\n",
        "  for i in tokens:\n",
        "    for j in i:\n",
        "      output += j + \" \"\n",
        "\n",
        "  return output\n",
        "\n",
        "output = create_single_string(tokens)\n",
        "output"
      ],
      "metadata": {
        "id": "QWYBh5W8to1Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#WordCloud#"
      ],
      "metadata": {
        "id": "2CtZ0TeWuijv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import wordcloud as wc\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "mycloud = wc.WordCloud().generate(output)\n",
        "plt.figure(figsize=(15,10))\n",
        "plt.imshow(mycloud)"
      ],
      "metadata": {
        "id": "hpzIUZ-Cuk02"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "[print(d) for d in bdocs]"
      ],
      "metadata": {
        "id": "lS3Qz5f7PnGc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(bdocs)\n",
        "# print(collection)"
      ],
      "metadata": {
        "id": "M5MiWNkufQqE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Dictionary#\n"
      ],
      "metadata": {
        "id": "VRNE5c_i5UNW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.corpora import Dictionary"
      ],
      "metadata": {
        "id": "XshER_NgeZbp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dictionary = Dictionary(bdocs)\n",
        "dictionary.filter_extremes(keep_n = 10000, no_above= 0.8, no_below = 20) #==> the collection is too small for filtering it\n",
        "for w in dictionary.values():\n",
        "  print(dictionary.token2id[w],w) # show word id and value"
      ],
      "metadata": {
        "id": "xy95At0GMWZL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#build a bag-of-words - corpus\n",
        "bagofwords = [dictionary.doc2bow(d) for d in bdocs]\n",
        "[print(d) for d in bagofwords]"
      ],
      "metadata": {
        "id": "mrH5PDsSMjas"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#just to show the matrix of bag-of-words\n",
        "uniquewords=set()\n",
        "for d in bdocs:\n",
        "  [uniquewords.add(w) for w in d]\n",
        "#create a dictionary for every doc with 0\n",
        "coldict=[]\n",
        "for d in bdocs:\n",
        "  coldict.append(dict.fromkeys(uniquewords,0))\n",
        "for i,d in enumerate(bdocs):\n",
        "  for w in d:\n",
        "    coldict[i][w]+=1\n",
        "columns=list(uniquewords)\n",
        "bagDoc=pd.DataFrame(coldict,columns=columns)\n",
        "bagDoc"
      ],
      "metadata": {
        "id": "L1eGUrIOMnMS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#just to show the tf-idf version\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "colAnt=[]\n",
        "for d in bdocs:\n",
        "  colAnt.append(' '.join(w for w in d))\n",
        "X = vectorizer.fit_transform(colAnt)\n",
        "features=vectorizer.get_feature_names()\n",
        "dense = X.todense()\n",
        "denseList=dense.tolist()\n",
        "dfTfIdf = pd.DataFrame(denseList, columns=set(features))\n",
        "dfTfIdf"
      ],
      "metadata": {
        "id": "rLWB_D5OMrD0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim import models\n",
        "from gensim.models import LdaModel\n",
        "tfidf = models.TfidfModel(bagofwords)\n",
        "corpus_tfidf = tfidf[bagofwords]\n",
        "# myModel = LdaModel(corpus = corpus_tfidf,num_topics= 3, random_state= 27644437, id2word = dictionary, alpha = 'auto',per_word_topics = True,passes = 100)\n",
        "# for t in myModel.show_topics():\n",
        "#   print(t)\n",
        "print('-----')\n",
        "myModel = LdaModel(corpus = bagofwords,num_topics= 10, random_state= 27644437, id2word = dictionary, alpha = 'auto',per_word_topics = True,passes = 100)\n",
        "for t in myModel.show_topics():\n",
        "  print(t)"
      ],
      "metadata": {
        "id": "9_pNgd6FMu-k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Docs Topics#"
      ],
      "metadata": {
        "id": "S3HDoDkkkxJo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i,d in enumerate(bagofwords): # collection and bagofwords must be synchronized\n",
        "  print(bdocs[i],':',myModel.get_document_topics(d,minimum_probability=0.3)) #threshold\n"
      ],
      "metadata": {
        "id": "ERWvTVmtM9jL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Topic Metrics#"
      ],
      "metadata": {
        "id": "6DqBc3VpktL4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import CoherenceModel  #c_v\n",
        "metrics=['u_mass', 'c_v', 'c_uci', 'c_npmi']\n",
        "for metric in metrics: #,'c_w2v']:\n",
        "  myModelCoher = CoherenceModel(model=myModel, texts= bdocs, dictionary=dictionary, coherence=metric)  \n",
        "  print('%-7s: %2.4f'%(metric,myModelCoher.get_coherence()))"
      ],
      "metadata": {
        "id": "vyOawULgO4Cz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tops=[]\n",
        "for k in range(10):\n",
        "    t=[]\n",
        "    for wt in myModel.show_topic(k):\n",
        "      t.append(wt[0])\n",
        "    tops.append(t)\n",
        "for t in tops:\n",
        "  topic_coher=CoherenceModel(topics=[t], texts= bdocs, dictionary=dictionary, coherence=metrics[1])\n",
        "  print('topic:',t)\n",
        "  print('C_v:%.3f'%(topic_coher.get_coherence()))\n",
        "print(\"All:\")\n",
        "topic_coher=CoherenceModel(topics=tops, texts= bdocs, dictionary=dictionary, coherence=metrics[1])\n",
        "print('C_v:%.3f'%(topic_coher.get_coherence()))\n",
        "# top topics\n",
        "print('Topics with the highest coherence score the coherence for each topic.')\n",
        "myModel.top_topics(corpus=bagofwords,dictionary=dictionary,coherence=metrics[1],texts=dlemma,topn=5)"
      ],
      "metadata": {
        "id": "JGNKfHL_PBJ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "QUTMPf_Rkq1Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Topics Similarity#"
      ],
      "metadata": {
        "id": "0HGsJUuUppao"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Build a dictionary mapping words to ids\n",
        "#topics_list: list of topics with probabilites\n",
        "#dart is the resulting dictionary\n",
        "def getTopicsDict(topics_list,dart):\n",
        "\tsjm=set()\n",
        "\t[[sjm.add(w[0]) for w in t] for t in topics_list]\n",
        "\tnextid=len(dart)\n",
        "\tfor k in sjm:\n",
        "\t\ttry:\n",
        "\t\t\tdart[k]\n",
        "\t\texcept:\n",
        "\t\t\tdart[k]=nextid\n",
        "\t\t\tnextid+=1\n",
        "##\n",
        "## based on the dictionary (topic_dict) create a new topic list with id\n",
        "def getTopicsBow(topics_list,topic_dict):\n",
        "\ttop_bow=[]\n",
        "\tfor l in topics_list:\n",
        "\t\tltup=[]\n",
        "\t\tfor t in l:\n",
        "\t\t\tltup.append((topic_dict[t[0]],t[1]))\n",
        "\t\ttop_bow.append(ltup)\n",
        "\treturn top_bow\n",
        "#\n",
        "def getSimTopics(topic,top_list_bow, dist):\n",
        "\tsimTop={}\n",
        "\tfor i,t in enumerate(top_list_bow):\n",
        "\t\tif dist==1:\n",
        "\t\t\tsimTop[i]=hellinger(t,topic)\n",
        "\t\tif dist==2:\n",
        "\t\t\tsimTop[i]=jaccard(topic,t) # returns an error :( - to check\n",
        "\t#sort dict\n",
        "\tssimTop={k: v for k, v in sorted(simTop.items(), key=lambda item: item[1])}\n",
        "\treturn ssimTop"
      ],
      "metadata": {
        "id": "bpohhHuwpmRx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}