# -*- coding: utf-8 -*-
"""LDA_fakenews.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fmeSrpXdXC-8c6brL-nr-upCy9k0XUA_

#NPL Fake News DataSet#

Lucas Jaenisch Lopes
"""

!pip install tomotopy

"""#Initial Imports#"""

import sys
import spacy
import pandas as pd
import tomotopy as tp
import numpy as np
import gensim  
import re
from sklearn.feature_extraction.text import CountVectorizer
from google.colab import drive

spacy.cli.download("en_core_web_md")
nlp = spacy.load('en_core_web_md')

"""#Load Documents#"""

drive.mount('/content/drive')
fpath='/content/drive/MyDrive/'
df=pd.read_csv(fpath+'Fake.csv')
df.columns

df.info()

df.drop('subject', inplace=True, axis=1)
df.drop('date', inplace=True, axis=1)
df.drop('title', inplace=True, axis=1)

"""#NLP#"""

def get_average_feature_length(docs, feature):
  total_size = 0
  max_size = 0
  min_size = 999

  # print(docs)
  # print(feature)

  for i in docs:
    length = len(i)
    total_size += length
  
    if length > max_size:
      max_size = length

    if length < min_size:
      min_size = length

  print(f"\n{'-'*15} {feature} {'-'*15}")
  print(f"Average length: {int(total_size / len(docs))}")
  print(f"Max length: {max_size}")
  print(f"Min length: {min_size}")
  print(f"Total length (not unique): {total_size}")
  return int(total_size / len(docs))

avg_text_size = get_average_feature_length(df['text'], 'text')

"""#HTML to plain text#

"""

from bs4 import BeautifulSoup as soup

clean_text = []
r = len(df)

for f in df.columns:
  if f == 'text':
    for i in df[f][:r]:
      clean_text.append((soup(i, "lxml").text, len(i)))

new_text = []

for i in range(r):
  if not clean_text[i][1] > avg_text_size:
    new_text.append(clean_text[i][0])

# df.head()

# Rejoin all remaining rows to the DataFrame
clean_df = pd.DataFrame()
clean_df['text'] = new_text

df = clean_df

"""#Stop Words#"""

spacy.cli.download("en_core_web_md")
nlp = spacy.load('en_core_web_md')

def convert_to_string(df, feature) -> str:
  output = []
  for i in range(len(df[feature])):
    output.append(df[feature][i].strip())
  return output

texts = convert_to_string(df, 'text')

dlemma = []
for i in texts:
  tdoc=nlp(i.lower())
  lm=" ".join([token.lemma_ for token in tdoc  if not(token.pos_ == 'NUM' or token.is_stop == True or token.is_digit == True or token.is_punct == True or token.lemma == False or token.like_email or  token.pos_ == 'VERB' or token.lemma_.startswith('@') or len(token.lemma_) < 3)])
  dlemma.append(lm)

for d1, d2 in zip(texts, dlemma):
  print(d1, '\n=>', d2)
  print()

"""#N grams#"""

from gensim.models.phrases import Phrases, Phraser

import gensim.utils

min_length = 4
dtoken = [gensim.utils.simple_preprocess(d, deacc=True, min_len=min_length) for d in dlemma]

for i in dtoken:
  if 'character' in i and 'offset' in i:
    print(i)
    
  # print(i)

bigram  = Phraser(Phrases(dtoken, min_count=5, threshold=15))
bdocs   = [bigram[d] for d in dtoken]


# [print(i) for i in bdocs]

print((bdocs)[:10])

bigrams = []

for i in bdocs:
  for j in i:
    bigrams.append(j)
    if '_' in j:
      print(j)

"""#Removing Dups#"""

bigramized = bigrams
uniques = set(bigramized)
uniques = dict(zip(range(len(uniques)), uniques))

print("Total bigrams: ", len(bigramized))
print("Unique bigrams: ",  len(uniques))

# uniques.values()

"""#Tokenize#"""

def tokenize(dlemma):
  max_size = len(dlemma); count = 0
  output = []
  
  for i in dlemma:
    # if count % 200 == 0:
      # print(f"{count}/{max_size}")
    
    aux = []; doc = nlp(i)

    for token in doc:
      if token.pos_ in ['NOUN', 'PROPN']:
        aux.append(token.lemma_)
    output.append(aux)
    count += 1
  return output
        
tokens = tokenize(uniques.values())
tokens[:10]

# remove empty objects from the tokens

print("Old number of Tokens: ", len(tokens))
clean_tokens = []
for t in tokens:
  if t != []:
    clean_tokens.append(t)
tokens = clean_tokens
print("New number of Tokens: ", len(tokens))

# [print(d) for d in tokens]

"""#Statistics#"""

number_of_docs = len(tokens)
number_of_docs

def stats_about_the_docs(docs, feature):
  total_size = 0
  max_size = 0
  min_size = 999

  for i in docs:
    length = len(i)
    total_size += length
  
    if length > max_size:
      max_size = length

    if length < min_size and length != 0:
      min_size = length
    
  print(f"{'-'*10} {feature} {'-'*10}")
  print(f"Average # of {feature}: {int(total_size / len(docs))}")
  print(f"Max # of {feature}: {max_size}")
  print(f"Min # of {feature}: {min_size}")

stats_about_the_docs(bdocs, 'text')

"""#WordCloud#"""

def create_single_string(tokens):
  output = ''
  for i in tokens:
    for j in i:
      output += j + " "

  return output

output = create_single_string(tokens)
# output

import wordcloud as wc
import matplotlib.pyplot as plt

mycloud = wc.WordCloud(max_words=1000,relative_scaling=1,width=1000,height=1000).generate(output)
plt.figure(figsize=(15,10))
plt.imshow(mycloud)

# [print(d) for d in bdocs]
# print(bdocs)
# print(collection)

from gensim.models.coherencemodel import CoherenceModel
from gensim.corpora import Dictionary
from gensim.models import LdaModel

"""#Bag of Words#"""

dictionary = Dictionary(bdocs)
dictionary.filter_extremes(keep_n = 10000, no_above= 0.8, no_below = len(bdocs)*.01)

corpus_bof=[dictionary.doc2bow(d) for d in bdocs]

"""#Hyper Parameters#"""

K=[20,25,30,35]
alpha=['symmetric']
eta=['symmetric']
passes=[100]
cv=[]
vocab = list(dictionary.values())
print('# of docs: %5d # of words: %6d'%(dictionary.num_docs, len(vocab)))
for k in K:
  for a in alpha:
    for b in eta:
      for p in passes:
        lda = LdaModel(corpus = corpus_bof,num_topics= k, random_state= 27644437, id2word = dictionary, alpha = a, eta=b,per_word_topics = True,passes = p)
        lda_cv = CoherenceModel(model=lda, texts= bdocs, dictionary=dictionary, coherence='c_v')
        cv_cohe=lda_cv.get_coherence()
        print('K: %2d alfa: %10s beta: %10s passes: %3d coherence: %.3f'%(k,a,b,p,cv_cohe))
        cv.append(cv_cohe)

plt.figure(figsize=(5,5))
plt.plot((20,25,30,35),cv,marker='o')
plt.grid(True)
plt.show()
print(max(zip(cv,list(K))))

# print(dictionary)
vocab = list(dictionary.values())
# [print(v) for v in dictionary.values()]
# print('# of docs: %5d # of words: %6d'%(dictionary.num_docs, len(vocab)))

"""#Topics#"""

best_number_of_topics = 25

cv =[]
lda = LdaModel(corpus = corpus_bof,num_topics= best_number_of_topics, random_state= 27644437, id2word = dictionary, alpha ='symmetric', eta='symmetric',per_word_topics = True,passes = p)
lda_cv = CoherenceModel(model=lda, texts= bdocs, dictionary=dictionary, coherence='c_v')
cv_cohe=lda_cv.get_coherence()
print('K: %2d alfa: %10s beta: %10s passes: %3d coherence: %.3f'%(k,a,b,p,cv_cohe))
cv.append(cv_cohe)

for i,d in enumerate(corpus_bof):
  print(bdocs[i],':',lda.get_document_topics(d,minimum_probability=0.4))

tops=[]
for k in range(best_number_of_topics):
    t=[]
    for wt in lda.show_topic(k):
      t.append(wt[0])
    tops.append(t)
for t in tops:
  topic_coher=CoherenceModel(topics=[t], texts= bdocs, dictionary=dictionary, coherence='c_v')
  print('topic:',t)
  print('C_v:%.3f'%(topic_coher.get_coherence()))
print("All:")
topic_coher=CoherenceModel(topics=tops, texts= bdocs, dictionary=dictionary, coherence='c_v')
print('C_v:%.3f'%(topic_coher.get_coherence()))
# top topics
print('Topics with the highest coherence score the coherence for each topic.')
lda.top_topics(corpus=corpus_bof,dictionary=dictionary,coherence='c_v',texts=bdocs,topn=5)

"""#Hottest Topics#"""

plt.figure(figsize=(15,10))
tops_frequency=[0]*best_number_of_topics
tops_f=[0]*25
x=0

for d in corpus_bof:
  aux=lda.get_document_topics(d,minimum_probability=0.4)
  if aux!=[]:
    tops_frequency[aux[0][0]]+=1
plt.subplot(1, 2, 1)
plt.bar(range(best_number_of_topics),tops_frequency)

for i in range(best_number_of_topics):
  aux1=[tops_frequency[i],i]
  tops_frequency[i]=aux1
tops_frequency.sort()
tops_frequency.reverse()
# print(tops_frequency)

aux2=[0]*10
for i in range(10):
  aux2[i]=tops_frequency[i][1]
print(aux2)
plt.show()

for i in aux2:
  print(lda.show_topics(num_topics=best_number_of_topics)[i])

"""#Labeling#"""

ddd=[]
for j in aux2:
  ddd2=[]
  for i,d in enumerate(corpus_bof):
    if len(ddd)<10:
      aux=lda.get_document_topics(d,minimum_probability=0.5)
    else:
      aux=lda.get_document_topics(d,minimum_probability=0.3)
    if aux!=[]:
      if aux[0][0]==j:
        ddd2.append(i)
        # print(aux[0],':',i,bdocs[i])
  ddd.append(ddd2)

for i in ddd:
  print(len(i),end=" ")

df.head()

pd.set_option('display.max_rows', None)
pd.set_option('display.max_columns', None)
pd.set_option('display.width', None)
pd.set_option('display.max_colwidth', 1)

lables=[]

"""#Topic 1#"""

print(lda.show_topics(num_topics=best_number_of_topics)[aux2[0]])

noticiasaux=df.iloc[ddd[0]]
noticiasaux.head()

lables.append("Police Force")

"""#Topic 2#"""

print(lda.show_topics(num_topics=best_number_of_topics)[aux2[1]])

noticiasaux=df.iloc[ddd[1]]
noticiasaux.head()

lables.append("Hillary Clinton Campaign")

"""#Topic 3#"""

print(lda.show_topics(num_topics=best_number_of_topics)[aux2[2]])

noticiasaux=df.iloc[ddd[2]]
noticiasaux.head()

lables.append("Donald Trump Campaign")

"""#Topic 4#"""

print(lda.show_topics(num_topics=best_number_of_topics)[aux2[3]])

noticiasaux=df.iloc[ddd[3]]
noticiasaux.head()

lables.append("Public Opinion")

"""#Topic 5#"""

print(lda.show_topics(num_topics=best_number_of_topics)[aux2[4]])

noticiasaux=df.iloc[ddd[4]]
noticiasaux.head()

lables.append("Donald Trump adminstration as President")

"""#Topic 6#"""

print(lda.show_topics(num_topics=best_number_of_topics)[aux2[5]])

noticiasaux=df.iloc[ddd[5]]
noticiasaux.head()

lables.append("Radio Propaganda")

"""#Topic 7#"""

print(lda.show_topics(num_topics=best_number_of_topics)[aux2[6]])

noticiasaux=df.iloc[ddd[6]]
noticiasaux.head()

lables.append("Hillary Clinton Performance during Ellections")

"""#Topic 8#"""

print(lda.show_topics(num_topics=best_number_of_topics)[aux2[7]])

noticiasaux=df.iloc[ddd[7]]
noticiasaux.head()

lables.append("Defense of Election results")

"""#Topic 9#"""

print(lda.show_topics(num_topics=best_number_of_topics)[aux2[8]])

noticiasaux=df.iloc[ddd[8]]
noticiasaux.head()

lables.append("International Policies")

"""#Topic 10#"""

print(lda.show_topics(num_topics=best_number_of_topics)[aux2[9]])

noticiasaux=df.iloc[ddd[9]]
noticiasaux.head()

lables.append("Media Outlets")

for i,j in zip(aux2,lables):
  print("\n", j,"\n",lda.show_topics(num_topics=25)[i])

for i in range(10):
  print(lables[i])